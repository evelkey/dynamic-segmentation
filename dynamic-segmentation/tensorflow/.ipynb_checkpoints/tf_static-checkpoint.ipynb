{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '§', '°', 'Á', 'É', 'Í', 'Ó', 'Ö', 'Ú', 'Ü', 'á', 'ä', 'é', 'ë', 'í', 'ó', 'ö', 'ú', 'ü', 'Ő', 'ő', 'ű']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import data\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "import tensorflow_fold as td\n",
    "from conv_lstm_cell import *\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size',    256, \"\"\"batchsize\"\"\")\n",
    "tf.app.flags.DEFINE_integer('epochs',        10, \"\"\"epoch count\"\"\")\n",
    "tf.app.flags.DEFINE_integer('truncate',      200, \"\"\"truncate input sequences to this length\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_dir',       \"/mnt/permanent/Home/nessie/velkey/data/\", \"\"\"data store basedir\"\"\")\n",
    "tf.app.flags.DEFINE_string('log_dir',        \"/mnt/permanent/Home/nessie/velkey/logs/\", \"\"\"logging directory root\"\"\")\n",
    "tf.app.flags.DEFINE_string('run_name',       \"ce_b256_mconv6_static_trun200_ADAM_lr005\", \"\"\"naming: loss_fn, batch size, architecture, optimizer\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_type',      \"sentence/\", \"\"\"can be sentence/, word/\"\"\")\n",
    "tf.app.flags.DEFINE_string('model',          \"lstm\", \"\"\"can be lstm, convlstm\"\"\")\n",
    "tf.app.flags.DEFINE_integer('stack_cells',   2, \"\"\"how many lstms to stack in each dimensions\"\"\")\n",
    "tf.app.flags.DEFINE_integer('cell_size',     1000, \"\"\"only valid with lstm model, size of the LSTM cell\"\"\")\n",
    "tf.app.flags.DEFINE_integer('conv_kernel',   0, \"\"\"convolutional kernel size for convlstm, if 0, vocab size is used\"\"\")\n",
    "tf.app.flags.DEFINE_integer('conv_channels', 64, \"\"\"convolutional output channels for convlstm\"\"\")\n",
    "tf.app.flags.DEFINE_string('loss',           \"crossentropy\", \"\"\"can be l1, l2, crossentropy\"\"\")\n",
    "tf.app.flags.DEFINE_string('optimizer',      \"ADAM\", \"\"\"can be ADAM, RMS, SGD\"\"\")\n",
    "tf.app.flags.DEFINE_float('learning_rate',   0.05, \"\"\"starting learning rate\"\"\")\n",
    "\n",
    "\n",
    "vocabulary = data.vocabulary(FLAGS.data_dir + 'vocabulary')\n",
    "vsize=len(vocabulary)\n",
    "print(vocabulary)\n",
    "\n",
    "index = lambda char: vocabulary.index(char)\n",
    "char = lambda i: vocabulary[i]\n",
    "\n",
    "\n",
    "class data():\n",
    "    def __init__(self, folder, truncate):\n",
    "        self.data_dir = folder\n",
    "        self.data = dict()\n",
    "        self.size = dict()\n",
    "        self.datasets = [\"train\", \"test\", \"validation\"]\n",
    "        self.truncate = truncate\n",
    "        \n",
    "        for dataset in self.datasets:\n",
    "            self.data[dataset] = self.sentence_reader(folder+dataset)\n",
    "            self.size[dataset] = sum(1 for line in open(folder+dataset))\n",
    "\n",
    "                        \n",
    "    def sentence_reader(self, file):\n",
    "        \"\"\"\n",
    "        read sentences from the data format setence: sentence\\tlabels\\n\n",
    "        \"\"\"\n",
    "        data = [line[:-1].split('\\t') for line in open(file)]\n",
    "        while True:\n",
    "            for item in data:\n",
    "                tags = [int(num) for num in item[1]]\n",
    "                if len(item[0]) == len(tags) and len(tags) != 0:\n",
    "                    sent_onehot = self.onehot(item[0])\n",
    "                    if len(sent_onehot) >= self.truncate:\n",
    "                        sent_onehot=sent_onehot[:self.truncate]\n",
    "                        tags = tags[:self.truncate]\n",
    "                    yield (sent_onehot, tags)    \n",
    "\n",
    "            \n",
    "    def onehot(self, string):\n",
    "        onehot = np.zeros([len(string),vsize])\n",
    "        indices = np.arange(len(string)), np.array([int(index(char)) for char in string])\n",
    "        onehot[indices]=1\n",
    "        return [onehot[i,:] for i in range(len(onehot))]\n",
    "\n",
    "    \n",
    "def model_information():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        print(variable.name, shape)\n",
    "        # print(len(shape))\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value\n",
    "        print(\"\\tparams: \", variable_parametes)\n",
    "        total_parameters += variable_parametes\n",
    "    print(total_parameters)\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = data(FLAGS.data_dir + FLAGS.data_type, FLAGS.truncate)\n",
    "def pad(record):\n",
    "    pads = ((FLAGS.truncate-len(record[1]), 0), (0, 0))\n",
    "    ins = np.pad(record[0], pad_width=pads, mode=\"constant\", constant_values=0)\n",
    "    outs = np.pad(record[1], pad_width=(FLAGS.truncate-len(record[1]), 0), mode=\"constant\", constant_values=0)\n",
    "    return (ins, outs)\n",
    "\n",
    "def get_padded_batch(dataset=\"train\"):\n",
    "    data = np.zeros([FLAGS.batch_size, FLAGS.truncate, vsize])\n",
    "    labels = np.zeros([FLAGS.batch_size, FLAGS.truncate, 1])\n",
    "    for i in range(FLAGS.batch_size):\n",
    "        sentence, label = pad(next(store.data[dataset]))\n",
    "        data[i] = sentence\n",
    "        labels[i, :, 0] = label\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-9620362347c1>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-9620362347c1>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    with tf.variable_scope(\"fw\"):\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, FLAGS.truncate, vsize))\n",
    "y = tf.placeholder(tf.int32, shape=(FLAGS.batch_size, FLAGS.truncate, 1))\n",
    "labels = y\n",
    "\n",
    "inputs =tf.expand_dims(x,-1)\n",
    "\n",
    "kernel_size = 20\n",
    "\n",
    "\n",
    "def convLSTM_cell(kernel_size, out_features, in_features=1):\n",
    "    convlstm = Conv1DLSTMCell(input_shape=[vsize, in_features], output_channels=out_features, kernel_shape=[kernel_size])\n",
    "    return convlstm\n",
    "\n",
    "def multi_convLSTM_cell(kernel_sizes, out_features):\n",
    "    in_features = [1, out_features[1:]]\n",
    "    return tf.contrib.rnn.MultiRNNCell(\n",
    "        [convLSTM_cell(kernel_sizes[i], out_features[i], in_features[i])\n",
    "         for i in range(len(kernel_sizes))])\n",
    "\n",
    "with tf.variable_scope(\"fw\"):\n",
    "    fw_cells = multi_convLSTM_cell([vsize]*3, [10,10,1])\n",
    "with tf.variable_scope(\"bw\"):\n",
    "    bw_cells = multi_convLSTM_cell([vsize]*3, [10,10,1])\n",
    "\n",
    "outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cells, cell_bw=bw_cells, inputs=inputs, dtype=tf.float32)\n",
    "\n",
    "RNN_out = tf.concat(outputs, -1)\n",
    "RNN_out = tf.squeeze(RNN_out)\n",
    "\n",
    "RNN_out = tf.reshape(RNN_out,(-1,FLAGS.truncate, vsize*2))\n",
    "\n",
    "filters = tf.get_variable(shape=[1,vsize * 2,1], dtype=tf.float32, name=\"filters\")\n",
    "bias = tf.get_variable(shape=[1], dtype=tf.float32, name=\"bias\")\n",
    "\n",
    "logits = tf.nn.conv1d(RNN_out,filters=filters,stride=1,padding='SAME') + bias\n",
    "\n",
    "predictions = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_chars_in_batch = tf.reduce_sum(x)\n",
    "all_chars_in_batch = tf.size(x) / vsize\n",
    "valid_ratio = valid_chars_in_batch / tf.cast(all_chars_in_batch, tf.float32)\n",
    "\n",
    "l1_loss = tf.reduce_mean(tf.abs(tf.subtract(logits, tf.cast(labels, tf.float32))))\n",
    "l2_loss = tf.reduce_mean(tf.square(tf.subtract(logits, tf.cast(labels, tf.float32))))\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.cast(labels, tf.float32)))\n",
    "\n",
    "if FLAGS.loss == \"l1\":\n",
    "    loss = l1_loss\n",
    "elif FLAGS.loss == \"l2\":\n",
    "    loss = l2_loss\n",
    "elif FLAGS.loss == \"crossentropy\":\n",
    "    loss = cross_entropy\n",
    "else:\n",
    "    raise NotImplemented\n",
    "\n",
    "path = FLAGS.log_dir + FLAGS.run_name\n",
    "writer = tf.summary.FileWriter(path, graph=tf.get_default_graph())\n",
    "saver = tf.train.Saver(max_to_keep=20)\n",
    "tf.summary.scalar(\"batch_loss\", loss)\n",
    "\n",
    "def metrics(probs, labels, x):\n",
    "    labels = tf.cast(labels, tf.int32)\n",
    "    predicted = tf.cast(tf.less(0.5, probs),tf.int32)\n",
    "    length = tf.reduce_sum(x)\n",
    "    \n",
    "    #crop the sequences:\n",
    "    labels = labels\n",
    "    \n",
    "    TP = tf.count_nonzero(predicted * labels)\n",
    "    TN = tf.count_nonzero((predicted - 1) * (labels - 1))\n",
    "    FP = tf.count_nonzero(predicted * (labels - 1))\n",
    "    FN = tf.count_nonzero((predicted - 1) * labels)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "                          \n",
    "    accuracy = tf.cast(tf.count_nonzero(tf.equal(predicted, labels)), tf.float32) / tf.cast(tf.size(labels), tf.float32) * 100\n",
    "         \n",
    "    return precision, recall, accuracy, f1\n",
    "    \n",
    "precision, recall, accuracy, f1 = metrics(predictions, labels,x)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "tf.summary.scalar(\"precision\", precision)\n",
    "tf.summary.scalar(\"recall\", recall)\n",
    "tf.summary.scalar(\"f1\", f1)\n",
    "         \n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "if FLAGS.optimizer == \"ADAM\":\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "elif FLAGS.optimizer == \"RMS\":\n",
    "    opt = tf.train.RMSPropOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "elif FLAGS.optimizer == \"SGD\":\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "else:\n",
    "    raise NotImplemented\n",
    "\n",
    "train_op = opt.minimize(loss)\n",
    "\n",
    "validation_loss_placeholder = tf.placeholder(tf.float32, name=\"validation\")\n",
    "validation_loss_summary = tf.summary.scalar('validation_loss', validation_loss_placeholder)\n",
    "validation_accuracy_placeholder = tf.placeholder(tf.float32, name=\"validation_accuracy\")\n",
    "validation_accuracy_summary = tf.summary.scalar('validation_accuracy', validation_accuracy_placeholder)\n",
    "validation_f1_placeholder = tf.placeholder(tf.float32, name=\"validation_f1\")\n",
    "validation_f1_summary = tf.summary.scalar('validation_f1', validation_f1_placeholder)\n",
    "test_loss_placeholder = tf.placeholder(tf.float32, name=\"test\")\n",
    "test_loss_summary = tf.summary.scalar('test_loss', test_loss_placeholder)\n",
    "test_f1_placeholder = tf.placeholder(tf.float32, name=\"test_f1\")\n",
    "test_f1_summary = tf.summary.scalar('test_f1', test_f1_placeholder)\n",
    "test_accuracy_placeholder = tf.placeholder(tf.float32, name=\"test_accuracy\")\n",
    "test_accuracy_summary = tf.summary.scalar('test_accuracy', test_accuracy_placeholder)\n",
    "\n",
    "def get_metrics_on_dataset(dataset, train_step):\n",
    "    losses = []\n",
    "    accs = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    step = int(store.size[dataset] / FLAGS.batch_size)\n",
    "    for i in tqdm.trange(step):\n",
    "        x_, y_ = get_padded_batch(dataset)\n",
    "        feed = {\n",
    "            x: x_,\n",
    "            y: y_}\n",
    "        batch_loss, acc, rec, f = sess.run([loss, accuracy, recall, f1],feed_dict=feed)\n",
    "        losses.append(batch_loss)\n",
    "        accs.append(acc)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f)\n",
    "    \n",
    "    l, a, r, f = np.average(losses), np.average(accs), np.average(recalls), np.average(f1s)\n",
    "    \n",
    "    if dataset == \"validation\":\n",
    "        feed = {validation_loss_placeholder: l,\n",
    "                validation_accuracy_placeholder: float(a),\n",
    "                validation_f1_placeholder: f}\n",
    "        vl, va, vf = sess.run([validation_loss_summary, validation_accuracy_summary, validation_f1_summary],feed_dict=feed)\n",
    "        writer.add_summary(vl, train_step)\n",
    "        writer.add_summary(va, train_step)\n",
    "        writer.add_summary(vf, train_step)\n",
    "    elif dataset == \"test\":\n",
    "        feed = {test_loss_placeholder: l,\n",
    "                test_accuracy_placeholder: float(a),\n",
    "                test_f1_placeholder: f}\n",
    "        vl, va, vf = sess.run([test_loss_summary, test_accuracy_summary, test_f1_summary],feed_dict=feed)\n",
    "        writer.add_summary(vl, train_step)\n",
    "        writer.add_summary(va, train_step)\n",
    "        writer.add_summary(vf, train_step)\n",
    "        writer.flush()\n",
    "\n",
    "    return l,a,r,f\n",
    "    \n",
    "    \n",
    "class stopper():\n",
    "    def __init__(self, patience=20):\n",
    "        self.log = []\n",
    "        self.patience = patience\n",
    "        self.should_stop = False\n",
    "        \n",
    "    def add(self, value):\n",
    "        self.log.append(value)\n",
    "        return self.check()\n",
    "    \n",
    "    def check(self):\n",
    "        minimum = min(self.log)\n",
    "        errors = sum([1 if i>minimum else 0 for i in self.log[self.log.index(minimum):]])\n",
    "        if errors > self.patience:\n",
    "            self.should_stop = True\n",
    "        return self.should_stop\n",
    "    \n",
    "early = stopper(20)\n",
    "steps = FLAGS.epochs * int(store.size[\"train\"] / FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2830 [00:00<?, ?batches/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 1/15 [00:00<00:06,  2.26it/s]\u001b[A\n",
      " 13%|█▎        | 2/15 [00:00<00:05,  2.38it/s]\u001b[A\n",
      " 20%|██        | 3/15 [00:01<00:04,  2.43it/s]\u001b[A\n",
      " 27%|██▋       | 4/15 [00:01<00:04,  2.45it/s]\u001b[A\n",
      " 33%|███▎      | 5/15 [00:02<00:04,  2.47it/s]\u001b[A\n",
      " 40%|████      | 6/15 [00:02<00:03,  2.47it/s]\u001b[A\n",
      " 47%|████▋     | 7/15 [00:02<00:03,  2.48it/s]\u001b[A\n",
      " 53%|█████▎    | 8/15 [00:03<00:02,  2.49it/s]\u001b[A\n",
      " 60%|██████    | 9/15 [00:03<00:02,  2.49it/s]\u001b[A\n",
      " 67%|██████▋   | 10/15 [00:04<00:02,  2.49it/s]\u001b[A\n",
      " 73%|███████▎  | 11/15 [00:04<00:01,  2.49it/s]\u001b[A\n",
      " 80%|████████  | 12/15 [00:04<00:01,  2.49it/s]\u001b[A\n",
      " 87%|████████▋ | 13/15 [00:05<00:00,  2.50it/s]\u001b[A\n",
      " 93%|█████████▎| 14/15 [00:05<00:00,  2.50it/s]\u001b[A\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.50it/s]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.514266  accuracy:  70.4514 % recall:  0.234419979593 fscore:  0.0636451025954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 141/2830 [02:13<42:19,  1.06batches/s] \n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 1/15 [00:00<00:05,  2.45it/s]\u001b[A\n",
      " 13%|█▎        | 2/15 [00:00<00:05,  2.46it/s]\u001b[A\n",
      " 20%|██        | 3/15 [00:01<00:04,  2.46it/s]\u001b[A\n",
      " 27%|██▋       | 4/15 [00:01<00:04,  2.46it/s]\u001b[A\n",
      " 33%|███▎      | 5/15 [00:02<00:04,  2.46it/s]\u001b[A\n",
      " 40%|████      | 6/15 [00:02<00:03,  2.46it/s]\u001b[A\n",
      " 47%|████▋     | 7/15 [00:02<00:03,  2.47it/s]\u001b[A\n",
      " 53%|█████▎    | 8/15 [00:03<00:02,  2.46it/s]\u001b[A\n",
      " 60%|██████    | 9/15 [00:03<00:02,  2.46it/s]\u001b[A\n",
      " 67%|██████▋   | 10/15 [00:04<00:02,  2.46it/s]\u001b[A\n",
      " 73%|███████▎  | 11/15 [00:04<00:01,  2.46it/s]\u001b[A\n",
      " 80%|████████  | 12/15 [00:04<00:01,  2.46it/s]\u001b[A\n",
      " 87%|████████▋ | 13/15 [00:05<00:00,  2.46it/s]\u001b[A\n",
      " 93%|█████████▎| 14/15 [00:05<00:00,  2.46it/s]\u001b[A\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.46it/s]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.167446  accuracy:  95.7021 % recall:  0.0 fscore:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 282/2830 [04:26<40:04,  1.06batches/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 1/15 [00:00<00:05,  2.48it/s]\u001b[A\n",
      " 13%|█▎        | 2/15 [00:00<00:05,  2.48it/s]\u001b[A\n",
      " 20%|██        | 3/15 [00:01<00:04,  2.49it/s]\u001b[A\n",
      " 27%|██▋       | 4/15 [00:01<00:04,  2.50it/s]\u001b[A\n",
      " 33%|███▎      | 5/15 [00:02<00:04,  2.50it/s]\u001b[A\n",
      " 40%|████      | 6/15 [00:02<00:03,  2.50it/s]\u001b[A\n",
      " 47%|████▋     | 7/15 [00:02<00:03,  2.50it/s]\u001b[A\n",
      " 53%|█████▎    | 8/15 [00:03<00:02,  2.50it/s]\u001b[A\n",
      " 60%|██████    | 9/15 [00:03<00:02,  2.50it/s]\u001b[A\n",
      " 67%|██████▋   | 10/15 [00:03<00:01,  2.50it/s]\u001b[A\n",
      " 73%|███████▎  | 11/15 [00:04<00:01,  2.50it/s]\u001b[A\n",
      " 80%|████████  | 12/15 [00:04<00:01,  2.50it/s]\u001b[A\n",
      " 87%|████████▋ | 13/15 [00:05<00:00,  2.50it/s]\u001b[A\n",
      " 93%|█████████▎| 14/15 [00:05<00:00,  2.50it/s]\u001b[A\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.50it/s]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.167213  accuracy:  95.6677 % recall:  0.0 fscore:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 314/2830 [05:01<40:16,  1.04batches/s]"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "for i in tqdm.trange(steps, unit=\"batches\"):\n",
    "    b_data, b_label = get_padded_batch(\"train\")\n",
    "    _, batch_loss, summary= sess.run([train_op, loss, summary_op], {x: b_data, y: b_label})\n",
    "    assert not np.isnan(batch_loss)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        writer.add_summary(summary, i)\n",
    "        \n",
    "    if i % int(steps / FLAGS.epochs / 2) == 0:\n",
    "        l, a, r, f = get_metrics_on_dataset(\"validation\", i)\n",
    "        print(\"loss: \", l, \" accuracy: \", a, \"% recall: \", r, \"fscore: \", f)\n",
    "        if early.add(l):\n",
    "            break\n",
    "            \n",
    "    if i % int(steps / FLAGS.epochs / 2) == 0:\n",
    "        save_path = saver.save(sess, path + \"/model.ckpt\", global_step=i)\n",
    "        \n",
    "print(\"Testing...\")\n",
    "l, a, r, f = get_metrics_on_dataset(\"test\", steps)\n",
    "print(\"loss: \", l, \" accuracy: \", a, \"% recall: \", r, \"fscore\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
