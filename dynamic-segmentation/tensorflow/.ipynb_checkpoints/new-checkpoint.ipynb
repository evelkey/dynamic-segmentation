{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynet segmentation with tf fold\n",
    "![animation](../../fold/tensorflow_fold/g3doc/animation.gif)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a bunch of fun\n",
    "import numpy as np\n",
    "import six\n",
    "import time\n",
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "import data\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "import tensorflow_fold as td\n",
    "from conv_lstm_cell import *\n",
    "\n",
    "# params\n",
    "EMBEDDING_SIZE = 64\n",
    "SEP = \"|\"\n",
    "BATCH_SIZE = 100\n",
    "data_dir = \"/mnt/permanent/Home/nessie/velkey/data/\"\n",
    "\n",
    "#our alphabet\n",
    "\n",
    "vocabulary = data.vocabulary(data_dir + 'vocabulary')\n",
    "vsize=len(vocabulary)\n",
    "print(vocabulary)\n",
    "\n",
    "index = lambda char: vocabulary.index(char)\n",
    "char = lambda i: vocabulary[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data():\n",
    "    def __init__(self, folder, truncate=100):\n",
    "        self.data_dir = folder\n",
    "        self.data = dict()\n",
    "        self.size = dict()\n",
    "        self.datasets = [\"train\", \"test\", \"validation\"]\n",
    "        self.truncate = truncate\n",
    "        \n",
    "        for dataset in self.datasets:\n",
    "            self.data[dataset] = self.sentence_reader(folder+dataset)\n",
    "            #self.size[dataset] = sum(1 for line in open(folder+dataset))\n",
    "\n",
    "                        \n",
    "    def sentence_reader(self, file):\n",
    "        \"\"\"\n",
    "        read sentences from the data format setence: word\\tword\\n.....\\t\\n\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            i=0\n",
    "            sentence = []\n",
    "            end_sentence = False\n",
    "            with open(file) as f:\n",
    "                for lines in f:\n",
    "                    line = lines[:-1].split('\\t')\n",
    "                    if line[0] != \"\":\n",
    "                        sentence.append(line)\n",
    "                    else:\n",
    "                        end_sentence = True\n",
    "                        i+=1\n",
    "                    if end_sentence:\n",
    "                        end_sentence = False\n",
    "                        sent = \" \".join([word[0] for word in sentence])\n",
    "                        segmented = \" \".join([word[1].replace(\" \",\"|\") for word in sentence])\n",
    "                        tags = []\n",
    "                        last_char = \"_\"\n",
    "                        for char in segmented:\n",
    "                            if char != \"|\":\n",
    "                                tags.append(0 if last_char!=\"|\" else 1)\n",
    "                            last_char = char\n",
    "                        if len(sent) != 0:\n",
    "                            sent_onehot = self.onehot(sent)\n",
    "                            if len(sent_onehot) == len(tags):\n",
    "                                if len(sent_onehot) >= self.truncate:\n",
    "                                    sent_onehot=sent_onehot[:self.truncate]\n",
    "                                    tags = tags[:self.truncate]\n",
    "                                yield (sent_onehot, tags)\n",
    "                            sentence = []      \n",
    "          \n",
    "            \n",
    "    def onehot(self, string):\n",
    "        onehot = np.zeros([len(string),vsize])\n",
    "        indices = np.arange(len(string)), np.array([int(index(char)) for char in string])\n",
    "        onehot[indices]=1\n",
    "        return [onehot[i,:] for i in range(len(onehot))]\n",
    "            \n",
    "store = data(\"/mnt/permanent/Home/nessie/velkey/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_info():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        print(variable.name, shape)\n",
    "        # print(len(shape))\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            # print(dim)\n",
    "            variable_parametes *= dim.value\n",
    "        print(\"\\tparams: \", variable_parametes)\n",
    "        total_parameters += variable_parametes\n",
    "    print(total_parameters)\n",
    "    return total_parameters\n",
    "\n",
    "def onehot(string):\n",
    "    onehot = np.zeros([len(string),vsize])\n",
    "    onehot[np.arange(len(string)), np.array([index(char) for char in string])]=1\n",
    "    return [onehot[i,:] for i in range(len(onehot))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convLSTM_cell(kernel_size, out_features = 64):\n",
    "    convlstm = Conv1DLSTMCell(input_shape=[vsize,1], output_channels=out_features, kernel_shape=[kernel_size])\n",
    "    return td.ScopedLayer(convlstm)\n",
    "\n",
    "def multi_convLSTM_cell(kernel_sizes, out_features):\n",
    "    stacked_convLSTM = tf.contrib.rnn.MultiRNNCell()\n",
    "    return td.ScopedLayer(stacked_convLSTM)\n",
    "    \n",
    "def bidirectional_dynamic_nn(fw_cell, bw_cell, out_features=64):\n",
    "    bidir_conv_lstm = td.Composition()\n",
    "    with bidir_conv_lstm.scope():        \n",
    "        fw_seq = td.Identity().reads(bidir_conv_lstm.input)\n",
    "        bw_seq = td.Slice(step=-1).reads(fw_seq)\n",
    "\n",
    "        forward_dir = (td.RNN(fw_cell) >> td.GetItem(0)).reads(fw_seq)\n",
    "        back_dir = (td.RNN(bw_cell) >> td.GetItem(0)).reads(bw_seq)\n",
    "        back_to_leftright = td.Slice(step=-1).reads(back_dir)\n",
    "        \n",
    "        output_transform = (td.Function(lambda x: tf.reshape(x, [-1,vsize*out_features])) >>\n",
    "                            #td.FC(10, activation=tf.nn.tanh) >>\n",
    "                            td.FC(1, activation=None))\n",
    "        \n",
    "        bidir_common = (td.ZipWith(td.Concat() >> \n",
    "                                  output_transform >> \n",
    "                                  td.Metric('logits'))).reads(forward_dir, back_to_leftright)\n",
    "                    \n",
    "        #tag_logits = td.Map(output_transform).reads(bidir_common)\n",
    "\n",
    "        bidir_conv_lstm.output.reads(bidir_common)\n",
    "    return bidir_conv_lstm\n",
    "\n",
    "\n",
    "data = td.Map(td.Vector(vsize) >> td.Function(lambda x: tf.reshape(x, [-1,vsize,1])))\n",
    "\n",
    "model =  data >> bidirectional_dynamic_nn(convLSTM_cell(vsize), convLSTM_cell(vsize)) >> td.Void()\n",
    "labels = td.Map(td.Scalar() >> td.Metric(\"labels\")) >> td.Void()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler = td.Compiler.create((model, labels))\n",
    "logits = tf.squeeze(compiler.metric_tensors['logits'])\n",
    "labels = compiler.metric_tensors['labels']\n",
    "\n",
    "loss = tf.reduce_mean(tf.abs(tf.subtract(labels,logits)))\n",
    "l2_loss = tf.reduce_mean(tf.abs(tf.subtract(labels,logits)))\n",
    "log_loss = (labels) * tf.log(logits) + (1 - labels) * tf.log(1 - logits)\n",
    "cross_entropy_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "\n",
    "\n",
    "preds = tf.nn.sigmoid(logits)\n",
    "\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op = opt.minimize(cross_entropy_loss)\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "feed = compiler.build_feed_dict([(onehot(\"naGon jó ötlet\"),[0,0,0,1,0,1,0,0,0,0,0,1,0,0]) for i in range(1)])\n",
    "#feed = compiler.build_feed_dict([next(store.data[\"train\"]) for _ in range(5)])\n",
    "for i in range(1000):\n",
    "    a,b,c,d= sess.run([preds, compiler.metric_tensors['labels'], loss, train_op], feed)\n",
    "    print(\"preds: \", a)\n",
    "    print(\"labels: \", b)\n",
    "    print(\"loss: \", c, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
