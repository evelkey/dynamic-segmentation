{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "%matplotlib inline\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from conv_lstm import ConvLSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data and build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convlstm module: CLSTM (\n",
      "  (cell_list): ModuleList (\n",
      "    (0): CLSTM_cell (\n",
      "      (conv): Conv2d(13, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (1): CLSTM_cell (\n",
      "      (conv): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "params:\n",
      "param  torch.Size([40, 13, 5, 5])\n",
      "mean  Variable containing:\n",
      "1.00000e-05 *\n",
      " -2.4437\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "param  torch.Size([40])\n",
      "mean  Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.0451\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "param  torch.Size([40, 20, 5, 5])\n",
      "mean  Variable containing:\n",
      "1.00000e-04 *\n",
      "  1.4647\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "param  torch.Size([40])\n",
      "mean  Variable containing:\n",
      "1.00000e-03 *\n",
      "  2.8384\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "hidden_h shape  2\n",
      "hidden_h shape  torch.Size([10, 10, 25, 25])\n",
      "out shape torch.Size([4, 10, 10, 25, 25])\n",
      "len hidden  2\n",
      "next hidden torch.Size([10, 10, 25, 25])\n",
      "convlstm dict odict_keys(['cell_list.0.conv.weight', 'cell_list.0.conv.bias', 'cell_list.1.conv.weight', 'cell_list.1.conv.bias'])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class CLSTM_cell(nn.Module):\n",
    "    \"\"\"Initialize a basic Conv LSTM cell.\n",
    "    Args:\n",
    "      shape: int tuple thats the height and width of the hidden states h and c()\n",
    "      filter_size: int that is the height and width of the filters\n",
    "      num_features: int thats the num of channels of the states, like hidden_size\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, shape, input_chans, filter_size, num_features):\n",
    "        super(CLSTM_cell, self).__init__()\n",
    "        \n",
    "        self.shape = shape#H,W\n",
    "        self.input_chans=input_chans\n",
    "        self.filter_size=filter_size\n",
    "        self.num_features = num_features\n",
    "        #self.batch_size=batch_size\n",
    "        self.padding=int((filter_size-1)/2)#in this way the output has the same size\n",
    "        self.conv = nn.Conv2d(self.input_chans + self.num_features, 4*self.num_features, self.filter_size, 1, self.padding)\n",
    "\n",
    "    \n",
    "    def forward(self, input, hidden_state):\n",
    "        hidden,c=hidden_state#hidden and c are images with several channels\n",
    "        #print 'hidden ',hidden.size()\n",
    "        #print 'input ',input.size()\n",
    "        combined = torch.cat((input, hidden), 1)#oncatenate in the channels\n",
    "        #print 'combined',combined.size()\n",
    "        A=self.conv(combined)\n",
    "        (ai,af,ao,ag)=torch.split(A,self.num_features,dim=1)#it should return 4 tensors\n",
    "        i=torch.sigmoid(ai)\n",
    "        f=torch.sigmoid(af)\n",
    "        o=torch.sigmoid(ao)\n",
    "        g=torch.tanh(ag)\n",
    "        \n",
    "        next_c=f*c+i*g\n",
    "        next_h=o*torch.tanh(next_c)\n",
    "        return next_h, next_c\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        return (Variable(torch.zeros(batch_size,self.num_features,self.shape[0],self.shape[1])).cuda(),Variable(torch.zeros(batch_size,self.num_features,self.shape[0],self.shape[1])).cuda())\n",
    "\n",
    "\n",
    "class CLSTM(nn.Module):\n",
    "    \"\"\"Initialize a basic Conv LSTM cell.\n",
    "    Args:\n",
    "      shape: int tuple thats the height and width of the hidden states h and c()\n",
    "      filter_size: int that is the height and width of the filters\n",
    "      num_features: int thats the num of channels of the states, like hidden_size\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, shape, input_chans, filter_size, num_features,num_layers):\n",
    "        super(CLSTM, self).__init__()\n",
    "        \n",
    "        self.shape = shape#H,W\n",
    "        self.input_chans=input_chans\n",
    "        self.filter_size=filter_size\n",
    "        self.num_features = num_features\n",
    "        self.num_layers=num_layers\n",
    "        cell_list=[]\n",
    "        cell_list.append(CLSTM_cell(self.shape, self.input_chans, self.filter_size, self.num_features).cuda())#the first\n",
    "        #one has a different number of input channels\n",
    "        \n",
    "        for idcell in range(1,self.num_layers):\n",
    "            cell_list.append(CLSTM_cell(self.shape, self.num_features, self.filter_size, self.num_features).cuda())\n",
    "        self.cell_list=nn.ModuleList(cell_list)      \n",
    "\n",
    "    \n",
    "    def forward(self, input, hidden_state):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            hidden_state:list of tuples, one for every layer, each tuple should be hidden_layer_i,c_layer_i\n",
    "            input is the tensor of shape seq_len,Batch,Chans,H,W\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        current_input = input.transpose(0, 1)#now is seq_len,B,C,H,W\n",
    "        #current_input=input\n",
    "        next_hidden=[]#hidden states(h and c)\n",
    "        seq_len=current_input.size(0)\n",
    "\n",
    "        \n",
    "        for idlayer in range(self.num_layers):#loop for every layer\n",
    "\n",
    "            hidden_c=hidden_state[idlayer]#hidden and c are images with several channels\n",
    "            all_output = []\n",
    "            output_inner = []            \n",
    "            for t in range(seq_len):#loop for every step\n",
    "                hidden_c=self.cell_list[idlayer](current_input[t,...],hidden_c)#cell_list is a list with different conv_lstms 1 for every layer\n",
    "\n",
    "                output_inner.append(hidden_c[0])\n",
    "\n",
    "            next_hidden.append(hidden_c)\n",
    "            current_input = torch.cat(output_inner, 0).view(current_input.size(0), *output_inner[0].size())#seq_len,B,chans,H,W\n",
    "\n",
    "\n",
    "        return next_hidden, current_input\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        init_states=[]#this is a list of tuples\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
    "        return init_states\n",
    "###########Usage#######################################    \n",
    "num_features=10\n",
    "filter_size=5\n",
    "batch_size=10\n",
    "shape=(25,25)#H,W\n",
    "inp_chans=3\n",
    "nlayers=2\n",
    "seq_len=4\n",
    "\n",
    "#If using this format, then we need to transpose in CLSTM\n",
    "input = Variable(torch.rand(batch_size,seq_len,inp_chans,shape[0],shape[1])).cuda()\n",
    "\n",
    "conv_lstm=CLSTM(shape, inp_chans, filter_size, num_features,nlayers)\n",
    "conv_lstm.apply(weights_init)\n",
    "conv_lstm.cuda()\n",
    "\n",
    "print('convlstm module:',conv_lstm)\n",
    "\n",
    "\n",
    "print('params:')\n",
    "params=conv_lstm.parameters()\n",
    "for p in params:\n",
    "   print ('param ',p.size())\n",
    "   print ('mean ',torch.mean(p))\n",
    "\n",
    "\n",
    "hidden_state=conv_lstm.init_hidden(batch_size)\n",
    "print('hidden_h shape ',len(hidden_state))\n",
    "print ('hidden_h shape ',hidden_state[0][0].size())\n",
    "out=conv_lstm(input,hidden_state)\n",
    "print ('out shape',out[1].size())\n",
    "print ('len hidden ', len(out[0]))\n",
    "print ('next hidden',out[0][0][0].size())\n",
    "print ('convlstm dict',conv_lstm.state_dict().keys())\n",
    "\n",
    "\n",
    "L=torch.sum(out[1])\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_1D_LSTM_cell(nn.Module):\n",
    "    \"\"\"Initialize a basic Conv LSTM cell.\n",
    "    Args:\n",
    "      shape: int, length of the hidden states h and c()\n",
    "      filter_size: int that is the height and width of the filters\n",
    "      num_features: int thats the num of channels of the states, like hidden_size\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, shape, input_chans, filter_size, num_features):\n",
    "        super(Conv_1D_LSTM_cell, self).__init__()\n",
    "        \n",
    "        self.shape = shape #\n",
    "        self.input_chans=input_chans\n",
    "        self.filter_size=filter_size\n",
    "        self.num_features = num_features\n",
    "        self.padding=int((filter_size-1)/2) #in this way the output has the same size\n",
    "        #self.conv = nn.Conv2d(self.input_chans + self.num_features, 4*self.num_features, self.filter_size, 1, self.padding)\n",
    "        self.conv = nn.Conv1d(self.input_chans + self.num_features, 4*self.num_features, self.filter_size, 1, self.padding)\n",
    "    \n",
    "    def forward(self, input, hidden_state):\n",
    "        hidden,c=hidden_state #hidden and c are images with several channels\n",
    "        #print 'hidden ',hidden.size()\n",
    "        #print 'input ',input.size()\n",
    "        combined = torch.cat((input, hidden), 1) #concatenate in the channels\n",
    "        #print 'combined',combined.size()\n",
    "        A=self.conv(combined)\n",
    "        (ai,af,ao,ag)=torch.split(A,self.num_features,dim=1)#it should return 4 tensors\n",
    "        i=torch.sigmoid(ai)\n",
    "        f=torch.sigmoid(af)\n",
    "        o=torch.sigmoid(ao)\n",
    "        g=torch.tanh(ag)\n",
    "        \n",
    "        next_c=f*c+i*g\n",
    "        next_h=o*torch.tanh(next_c)\n",
    "        return next_h, next_c\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        return (Variable(torch.zeros(batch_size,self.num_features,self.shape)).cuda(),Variable(torch.zeros(batch_size,self.num_features,self.shape)).cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiConvLSTM(nn.Module):\n",
    "    \n",
    "    \"\"\"Initialize a basic Conv LSTM cell.\n",
    "    Args:\n",
    "      shape: int thats the length of the hidden states h and c()\n",
    "      filter_size: int that is the height and width of the filters\n",
    "      num_features: int thats the num of channels of the states, like hidden_size\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, shape, input_chans, filter_size, num_features,num_layers):\n",
    "        super(MultiConvLSTM, self).__init__()\n",
    "        \n",
    "        self.shape = shape\n",
    "        self.input_chans=input_chans\n",
    "        self.filter_size=filter_size\n",
    "        self.num_features = num_features\n",
    "        self.num_layers=num_layers\n",
    "        cell_list=[]\n",
    "        cell_list.append(Conv_1D_LSTM_cell(self.shape, self.input_chans, self.filter_size, self.num_features).cuda())#the first\n",
    "        #one has a different number of input channels\n",
    "        \n",
    "        for idcell in range(1,self.num_layers):\n",
    "            cell_list.append(Conv_1D_LSTM_cell(self.shape, self.num_features, self.filter_size, self.num_features).cuda())\n",
    "        self.cell_list=nn.ModuleList(cell_list)      \n",
    "\n",
    "    \n",
    "    def forward(self, input, hidden_state):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            hidden_state:list of tuples, one for every layer, each tuple should be hidden_layer_i,c_layer_i\n",
    "            input is the tensor of shape Batch,seq_len,Channels,length\n",
    "        \"\"\"\n",
    "\n",
    "        current_input = input.transpose(0, 1)#now is seq_len,B,C,H,W\n",
    "        #current_input=input\n",
    "        next_hidden=[]#hidden states(h and c)\n",
    "        seq_len=current_input.size(0)\n",
    "\n",
    "        \n",
    "        for idlayer in range(self.num_layers):#loop for every layer\n",
    "\n",
    "            hidden_c=hidden_state[idlayer]#hidden and c are images with several channels\n",
    "            all_output = []\n",
    "            output_inner = []            \n",
    "            for t in range(seq_len):#loop for every step\n",
    "                hidden_c=self.cell_list[idlayer](current_input[t,...],hidden_c)#cell_list is a list with different conv_lstms 1 for every layer\n",
    "\n",
    "                output_inner.append(hidden_c[0])\n",
    "\n",
    "            next_hidden.append(hidden_c)\n",
    "            current_input = torch.cat(output_inner, 0).view(current_input.size(0), *output_inner[0].size())#seq_len,B,chans,H,W\n",
    "\n",
    "\n",
    "        return next_hidden, current_input\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        init_states=[]#this is a list of tuples\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size))\n",
    "        return init_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Usage#######################################    \n",
    "num_features=10\n",
    "filter_size=5\n",
    "batch_size=10\n",
    "shape=25\n",
    "inp_chans=3\n",
    "nlayers=2\n",
    "seq_len=4\n",
    "\n",
    "#If using this format, then we need to transpose in CLSTM\n",
    "input = Variable(torch.rand(batch_size,seq_len,inp_chans,shape[0],shape[1])).cuda()\n",
    "\n",
    "conv_lstm=CLSTM(shape, inp_chans, filter_size, num_features,nlayers)\n",
    "conv_lstm.apply(weights_init)\n",
    "conv_lstm.cuda()\n",
    "\n",
    "print('convlstm module:',conv_lstm)\n",
    "\n",
    "\n",
    "print('params:')\n",
    "params=conv_lstm.parameters()\n",
    "for p in params:\n",
    "   print ('param ',p.size())\n",
    "   print ('mean ',torch.mean(p))\n",
    "\n",
    "\n",
    "hidden_state=conv_lstm.init_hidden(batch_size)\n",
    "print('hidden_h shape ',len(hidden_state))\n",
    "print ('hidden_h shape ',hidden_state[0][0].size())\n",
    "out=conv_lstm(input,hidden_state)\n",
    "print ('out shape',out[1].size())\n",
    "print ('len hidden ', len(out[0]))\n",
    "print ('next hidden',out[0][0][0].size())\n",
    "print ('convlstm dict',conv_lstm.state_dict().keys())\n",
    "\n",
    "\n",
    "L=torch.sum(out[1])\n",
    "L.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
