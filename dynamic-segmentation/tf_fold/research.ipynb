{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynet segmentation with tf fold\n",
    "![animation](../../fold/tensorflow_fold/g3doc/animation.gif)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'C', 'D', 'E', 'G', 'L', 'N', 'S', 'T', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '²', '³', '¹', 'á', 'é', 'í', 'ó', 'ö', 'ú', 'ü', 'ő', 'ű']\n"
     ]
    }
   ],
   "source": [
    "#just a bunch of fun\n",
    "import numpy as np\n",
    "import six\n",
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "import data\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "import tensorflow_fold as td\n",
    "from conv_lstm_cell import *\n",
    "\n",
    "# params\n",
    "EMBEDDING_SIZE = 64\n",
    "SEP = \"|\"\n",
    "BATCH_SIZE = 100\n",
    "data_dir = \"/home/moon/data/\"\n",
    "\n",
    "#our alphabet\n",
    "\n",
    "vocabulary = data.vocabulary(data_dir + 'vocabulary')\n",
    "vsize=len(vocabulary)\n",
    "print(vocabulary)\n",
    "\n",
    "index = lambda char: vocabulary.index(char)\n",
    "char = lambda i: vocabulary[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reader():\n",
    "    def __init__(self, folder, mode=\"bidirectional\", qsize=10000, start=True):\n",
    "        self.data_dir = folder\n",
    "        self.qsize= qsize\n",
    "        self.pool = dict()\n",
    "        self.data = dict()\n",
    "        self.datasets = [\"train\", \"test\", \"validation\"]\n",
    "        if mode == \"bidirectional\":\n",
    "            convert_fn = self.bidirectional_sentence_reader\n",
    "        else:\n",
    "            convert_fn = self.sentence_reader\n",
    "            \n",
    "        for dataset in self.datasets:\n",
    "            self.data[dataset] = Queue(qsize)\n",
    "            self.pool[dataset] = Process(target=convert_fn, args=(self.data[dataset], self.data_dir+dataset))\n",
    "        if start == True:\n",
    "            self.start()\n",
    "            \n",
    "    def sentence_reader(self, queue, file):\n",
    "        \"\"\"\n",
    "        read sentences from the data format setence: word\\tword\\n.....\\t\\n\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            with open(file) as f:\n",
    "                while True:\n",
    "                    try:\n",
    "                        sentence = []\n",
    "                        while True:\n",
    "                            line = f.readline()[:-1].split('\\t')\n",
    "                            if line[0] != \"\":\n",
    "                                sentence.append(line)\n",
    "                            else:\n",
    "                                break\n",
    "                        sent = \" \".join([word[0] for word in sentence])\n",
    "                        segmented = \" \".join([word[1].replace(\" \",\"|\") for word in sentence])\n",
    "                        tags = []\n",
    "                        last_char = \"_\"\n",
    "                        for char in segmented:\n",
    "                            if char != \"|\":\n",
    "                                tags.append(0 if last_char!=\"|\" else 1)\n",
    "                            last_char = char\n",
    "                        queue.put((sent, tags))\n",
    "                    except e:\n",
    "                        print(e)\n",
    "                        \n",
    "    def bidirectional_sentence_reader(self, queue, file):\n",
    "        \"\"\"\n",
    "        read sentences from the data format setence: word\\tword\\n.....\\t\\n\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            with open(file) as f:\n",
    "                while True:\n",
    "                    try:\n",
    "                        sentence = []\n",
    "                        while True:\n",
    "                            line = f.readline()[:-1].split('\\t')\n",
    "                            if line[0] != \"\":\n",
    "                                sentence.append(line)\n",
    "                            else:\n",
    "                                break\n",
    "                        sent = \" \".join([word[0] for word in sentence])\n",
    "                        segmented = \" \".join([word[1].replace(\" \",\"|\") for word in sentence])\n",
    "                        tags = []\n",
    "                        last_char = \"_\"\n",
    "                        for char in segmented:\n",
    "                            if char != \"|\":\n",
    "                                tags.append(0 if last_char!=\"|\" else 1)\n",
    "                            last_char = char\n",
    "                        for i in range(1, len(sent)-1):\n",
    "                            forward, backward = sent[:i], sent[i:][::-1]\n",
    "                            queue.put(([self.onehot(forward), self.onehot(backward)], tags[i]))\n",
    "                    except:\n",
    "                        print(\"err\")\n",
    "                        \n",
    "    def start(self):\n",
    "        for dataset in self.datasets:\n",
    "            self.pool[dataset].start()\n",
    "            \n",
    "    def stop(self):\n",
    "        for dataset in self.datasets:\n",
    "            self.pool[dataset].terminate()\n",
    "            \n",
    "    def get(self,dataset):\n",
    "        if dataset in self.datasets:\n",
    "            return self.data[dataset].get()\n",
    "        else:\n",
    "            raise KeyError\n",
    "            \n",
    "    def onehot(self, string):\n",
    "        onehot = np.zeros([len(string),vsize])\n",
    "        onehot[np.arange(len(string)), np.array([index(char) for char in string])]=1\n",
    "        return [onehot[i,:] for i in range(len(onehot))]\n",
    "            \n",
    "store = reader(\"/home/moon/data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_info():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        print(variable.name, shape)\n",
    "        # print(len(shape))\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            # print(dim)\n",
    "            variable_parametes *= dim.value\n",
    "        print(\"\\tparams: \", variable_parametes)\n",
    "        total_parameters += variable_parametes\n",
    "    print(total_parameters)\n",
    "    return total_parameters\n",
    "\n",
    "def onehot(string):\n",
    "    onehot = np.zeros([len(string),vsize])\n",
    "    onehot[np.arange(len(string)), np.array([index(char) for char in string])]=1\n",
    "    return [onehot[i,:] for i in range(len(onehot))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cell = td.ScopedLayer(tf.contrib.rnn.BasicLSTMCell(num_units=16), 'char_cell')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_conv_LSTM():\n",
    "    convlstm = Conv1DLSTMCell(input_shape=[vsize,1], output_channels=8, kernel_shape=[5])\n",
    "    conv_lstm_cell_1d = td.ScopedLayer(convlstm)\n",
    "\n",
    "    bidir_conv_lstm = td.Composition()\n",
    "    with bidir_conv_lstm.scope():\n",
    "        \n",
    "        forward = td.Identity().reads(bidir_conv_lstm.input[0])\n",
    "        backward = td.Identity().reads(bidir_conv_lstm.input[1])\n",
    "\n",
    "        forw = (td.RNN(conv_lstm_cell_1d) >>\n",
    "                td.GetItem(1) >>\n",
    "                td.GetItem(0) >>\n",
    "                td.Function(lambda rnn_outs: tf.contrib.layers.flatten(rnn_outs))).reads(forward)\n",
    "\n",
    "        backw = (td.RNN(conv_lstm_cell_1d) >>\n",
    "                 td.GetItem(1) >>\n",
    "                 td.GetItem(0) >>\n",
    "                 td.Function(lambda rnn_outs: tf.contrib.layers.flatten(rnn_outs))).reads(backward)\n",
    "\n",
    "        rnn_outs = td.Concat().reads(forw,backw)\n",
    "        bidir_conv_lstm.output.reads(rnn_outs)\n",
    "    return bidir_conv_lstm >> td.FC(1)\n",
    "\n",
    "\n",
    "def FCNN():\n",
    "    return td.FC(50) >> td.FC(1)# >> td.Function(lambda xs: tf.squeeze(xs, axis=1))\n",
    "\n",
    "data = td.Record((td.Map(\n",
    "                        td.Vector(vsize) >>\n",
    "                        td.Function(lambda x: tf.reshape(x, [-1,vsize,1]))),\n",
    "                  td.Map(\n",
    "                        td.Vector(vsize) >>\n",
    "                        td.Function(lambda x: tf.reshape(x, [-1,vsize,1])))))\n",
    "bidir =  data >> bidirectional_conv_LSTM()\n",
    "#fc = FCNN()\n",
    "blk = bidir# >> fc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moon/Envs/tf1/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "compiler = td.Compiler.create((blk, td.Scalar()))\n",
    "model_output, target = compiler.output_tensors\n",
    "loss = tf.nn.l2_loss(model_output - target)\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_op = opt.minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(1000):\n",
    "    loff, _ = sess.run([loss, train_op], compiler.build_feed_dict([store.get(\"train\") for _ in range(100)]))\n",
    "    losses.append(loss)\n",
    "    if i%10==0:\n",
    "        print(loff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run([loss, train_op], compiler.build_feed_dict([[[onehot(\"aa\"), onehot(\"bbf\")],1] for _ in range(10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
