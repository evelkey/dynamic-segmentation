{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynet segmentation with tf fold\n",
    "![animation](../../fold/tensorflow_fold/g3doc/animation.gif)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'C', 'D', 'E', 'G', 'L', 'N', 'S', 'T', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '²', '³', '¹', 'á', 'é', 'í', 'ó', 'ö', 'ú', 'ü', 'ő', 'ű']\n"
     ]
    }
   ],
   "source": [
    "#just a bunch of fun\n",
    "import numpy as np\n",
    "import six\n",
    "from multiprocessing import Process, Queue\n",
    "import time\n",
    "import data\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "import tensorflow_fold as td\n",
    "from conv_lstm_cell import *\n",
    "\n",
    "# params\n",
    "EMBEDDING_SIZE = 64\n",
    "SEP = \"|\"\n",
    "data_dir = \"/home/moon/data/\"\n",
    "\n",
    "#our alphabet\n",
    "\n",
    "vocabulary = data.vocabulary(data_dir + 'vocabulary')\n",
    "vsize=len(vocabulary)\n",
    "print(vocabulary)\n",
    "\n",
    "index = lambda char: vocabulary.index(char)\n",
    "char = lambda i: vocabulary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(queue, file):\n",
    "    \"\"\"\n",
    "    read sentences from the data format setence: word\\tword\\n.....\\t\\n\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        with open(file) as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    sentence = []\n",
    "                    while True:\n",
    "                        line = f.readline()[:-1].split('\\t')\n",
    "                        if line[0] != \"\":\n",
    "                            sentence.append(line)\n",
    "                        else:\n",
    "                            break\n",
    "                    sent = \" \".join([word[0] for word in sentence])\n",
    "                    segmented = \" \".join([word[1].replace(\" \",\"|\") for word in sentence])\n",
    "                    tags = []\n",
    "                    last_char = \"_\"\n",
    "                    for char in segmented:\n",
    "                        if char != \"|\":\n",
    "                            tags.append(0 if last_char!=\"|\" else 1)\n",
    "                        last_char = char\n",
    "                    queue.put((sent, tags))\n",
    "                except e:\n",
    "                    print(e)\n",
    "\n",
    "qsize= 10000\n",
    "pool = dict()\n",
    "data = dict()\n",
    "for dataset in [\"train\", \"test\", \"validation\"]:\n",
    "    data[dataset] = Queue(qsize)\n",
    "    pool[dataset] = Process(target=reader, args=(data[dataset], data_dir+dataset))\n",
    "    pool[dataset].start()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sátoraljaújheL _ jókai út _', [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"].get())\n",
    "\n",
    "#p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_info():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        print(variable.name, shape)\n",
    "        # print(len(shape))\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            # print(dim)\n",
    "            variable_parametes *= dim.value\n",
    "        print(\"\\tparams: \", variable_parametes)\n",
    "        total_parameters += variable_parametes\n",
    "    print(total_parameters)\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'a', 'l', 'ú']\n"
     ]
    }
   ],
   "source": [
    "length = td.Length()\n",
    "\n",
    "embedded =  (td.InputTransform(lambda s: [index(x) for x in s]) >> \n",
    "             td.Map(td.Scalar(tf.int32) >> \n",
    "             td.Function(td.Embedding(vsize, EMBEDDING_SIZE))))\n",
    "\n",
    "onehot = (td.InputTransform(lambda s: [index(x) for x in s]) >>\n",
    "          td.Map(td.Scalar(tf.int32) >>\n",
    "          td.Function(lambda indices: tf.one_hot(indices, depth=vsize)) >>\n",
    "          td.Function(lambda x: tf.reshape(x, [-1,vsize,1]))))\n",
    "\n",
    "decode_onehot = td.InputTransform(lambda s: [char(np.argmax(np.squeeze(x))) for x in s])\n",
    "\n",
    "print(decode_onehot.eval(onehot.eval(\"malú\")))\n",
    "#embedded.eval(\"kacsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_on_sequence(x, scope, kernel_size=3, input_channels=72, output_channels=72):\n",
    "    with tf.variable_scope(scope) as sc:\n",
    "        filters = tf.get_variable(\"conv_filter\", [kernel_size] +  [input_channels, output_channels] , initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        bias = tf.get_variable(\"conv_bias\",  output_channels, initializer=tf.constant_initializer(0.05, dtype=tf.float32))\n",
    "        conv = tf.nn.conv1d(x, filters=filters, stride=1, padding='VALID')\n",
    "        return tf.nn.relu(tf.add(conv, bias))\n",
    "    \n",
    "def SeqToTuple(T, N):\n",
    "    return (td.InputTransform(lambda x: tuple(x))\n",
    "            .set_input_type(td.SequenceType(T))\n",
    "            .set_output_type(td.Tuple(*([T] * N))))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vec = td.Tensor([len(vocabulary)])\n",
    "data_segments = td.Map(vec) >> td.NGrams(3) \n",
    "\n",
    "b = td.Zeros([3,72]) >> td.ScopedLayer(conv1d_on_sequence)\n",
    "b.eval(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convlstm = Conv1DLSTMCell(input_shape=[vsize,1], output_channels=8, kernel_shape=[5])\n",
    "conv_lstm_cell_1d = td.ScopedLayer(convlstm)\n",
    "\n",
    "model = (td.Map(td.Tensor([vsize,1])) >>\n",
    "         td.RNN(conv_lstm_cell_1d) >>\n",
    "         td.GetItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode =  onehot >> td.RNN(conv_lstm_cell_1d)\n",
    "#out = encode.eval(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = td.Composition()\n",
    "with comp.scope():\n",
    "    #forward = td.Identity().reads(comp.input[0])\n",
    "    #backward = td.Identity().reads(comp.input[1])\n",
    "    model1 = (td.InputTransform(lambda s: [index(x) for x in s]) >>\n",
    "             td.Map(td.Scalar(tf.int32) >>\n",
    "             td.Function(lambda indices: tf.one_hot(indices, depth=vsize)) >>\n",
    "             td.Function(lambda x: tf.reshape(x, [-1,vsize,1]))) >>             #onehot encoding\n",
    "             td.RNN(conv_lstm_cell_1d)  >> td.GetItem(1) >> td.GetItem(0) ).reads(comp.input[0])\n",
    "    model2 = (td.InputTransform(lambda s: [index(x) for x in s]) >>\n",
    "             td.Map(td.Scalar(tf.int32) >>\n",
    "             td.Function(lambda indices: tf.one_hot(indices, depth=vsize)) >>\n",
    "             td.Function(lambda x: tf.reshape(x, [-1,vsize,1]))) >>             #onehot encoding\n",
    "             td.RNN(conv_lstm_cell_1d) >> td.GetItem(1) >> td.GetItem(0) ).reads(comp.input[1])\n",
    "    \n",
    "    rnn_outs = td.Concat().reads(model1, model2)#td.Function(lambda forward, backward: tf.concat([forward,backward],1)).reads(model1, model2)\n",
    "    out = td.Function(lambda rnn_outs: tf.contrib.layers.flatten(rnn_outs)).reads(rnn_outs)\n",
    "    #fc = out >> td.FC(300) >> td.FC(1)\n",
    "    comp.output.reads(out)\n",
    "    \n",
    "new = comp >> td.FC(300) >>td.FC(200) >> td.FC(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.eval((\"cucc\", \"valami\")).shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
