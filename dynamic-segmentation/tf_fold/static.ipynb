{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '§', '°', 'Á', 'É', 'Í', 'Ó', 'Ö', 'Ú', 'Ü', 'á', 'ä', 'é', 'ë', 'í', 'ó', 'ö', 'ú', 'ü', 'Ő', 'ő', 'ű']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import data\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "import tensorflow_fold as td\n",
    "from conv_lstm_cell import *\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size',    8, \"\"\"batchsize\"\"\")\n",
    "tf.app.flags.DEFINE_integer('epochs',        10, \"\"\"epoch count\"\"\")\n",
    "tf.app.flags.DEFINE_integer('truncate',      120, \"\"\"truncate input sequences to this length\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_dir',       \"/mnt/permanent/Home/nessie/velkey/data/\", \"\"\"data store basedir\"\"\")\n",
    "tf.app.flags.DEFINE_string('log_dir',        \"/mnt/permanent/Home/nessie/velkey/logs/\", \"\"\"logging directory root\"\"\")\n",
    "tf.app.flags.DEFINE_string('run_name',       \"development\", \"\"\"naming: loss_fn, batch size, architecture, optimizer\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_type',      \"sentence/\", \"\"\"can be sentence/, word/\"\"\")\n",
    "tf.app.flags.DEFINE_string('model',          \"lstm\", \"\"\"can be lstm, convlstm\"\"\")\n",
    "tf.app.flags.DEFINE_integer('stack_cells',   2, \"\"\"how many lstms to stack in each dimensions\"\"\")\n",
    "tf.app.flags.DEFINE_integer('cell_size',     1000, \"\"\"only valid with lstm model, size of the LSTM cell\"\"\")\n",
    "tf.app.flags.DEFINE_integer('conv_kernel',   0, \"\"\"convolutional kernel size for convlstm, if 0, vocab size is used\"\"\")\n",
    "tf.app.flags.DEFINE_integer('conv_channels', 64, \"\"\"convolutional output channels for convlstm\"\"\")\n",
    "tf.app.flags.DEFINE_string('loss',           \"crossentropy\", \"\"\"can be l1, l2, crossentropy\"\"\")\n",
    "tf.app.flags.DEFINE_string('optimizer',      \"ADAM\", \"\"\"can be ADAM, RMS, SGD\"\"\")\n",
    "tf.app.flags.DEFINE_float('learning_rate',   0.001, \"\"\"starting learning rate\"\"\")\n",
    "\n",
    "\n",
    "vocabulary = data.vocabulary(FLAGS.data_dir + 'vocabulary')\n",
    "vsize=len(vocabulary)\n",
    "print(vocabulary)\n",
    "\n",
    "index = lambda char: vocabulary.index(char)\n",
    "char = lambda i: vocabulary[i]\n",
    "\n",
    "\n",
    "class data():\n",
    "    def __init__(self, folder, truncate):\n",
    "        self.data_dir = folder\n",
    "        self.data = dict()\n",
    "        self.size = dict()\n",
    "        self.datasets = [\"train\", \"test\", \"validation\"]\n",
    "        self.truncate = truncate\n",
    "        \n",
    "        for dataset in self.datasets:\n",
    "            self.data[dataset] = self.sentence_reader(folder+dataset)\n",
    "            self.size[dataset] = sum(1 for line in open(folder+dataset))\n",
    "\n",
    "                        \n",
    "    def sentence_reader(self, file):\n",
    "        \"\"\"\n",
    "        read sentences from the data format setence: sentence\\tlabels\\n\n",
    "        \"\"\"\n",
    "        data = [line[:-1].split('\\t') for line in open(file)]\n",
    "        while True:\n",
    "            for item in data:\n",
    "                tags = [int(num) for num in item[1]]\n",
    "                if len(item[0]) == len(tags) and len(tags) != 0:\n",
    "                    sent_onehot = self.onehot(item[0])\n",
    "                    if len(sent_onehot) >= self.truncate:\n",
    "                        sent_onehot=sent_onehot[:self.truncate]\n",
    "                        tags = tags[:self.truncate]\n",
    "                    yield (sent_onehot, tags)    \n",
    "\n",
    "            \n",
    "    def onehot(self, string):\n",
    "        onehot = np.zeros([len(string),vsize])\n",
    "        indices = np.arange(len(string)), np.array([int(index(char)) for char in string])\n",
    "        onehot[indices]=1\n",
    "        return [onehot[i,:] for i in range(len(onehot))]\n",
    "\n",
    "    \n",
    "def model_information():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        print(variable.name, shape)\n",
    "        # print(len(shape))\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value\n",
    "        print(\"\\tparams: \", variable_parametes)\n",
    "        total_parameters += variable_parametes\n",
    "    print(total_parameters)\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def convLSTM_cell(kernel_size, out_features = 64):\n",
    "    convlstm = Conv1DLSTMCell(input_shape=[vsize,1], output_channels=out_features, kernel_shape=[kernel_size])\n",
    "    return td.ScopedLayer(convlstm)\n",
    "\n",
    "def multi_convLSTM_cell(kernel_sizes, out_features):\n",
    "    return td.ScopedLayer(tf.contrib.rnn.MultiRNNCell(\n",
    "        [convLSTM_cell(kernel, features)\n",
    "         for (kernel, features) in zip(kernel_sizes, out_features)]))\n",
    "\n",
    "def FC_cell(units):\n",
    "    return td.ScopedLayer(tf.contrib.rnn.LSTMCell(num_units=units))\n",
    "\n",
    "def multi_FC_cell(units_list):\n",
    "    return td.ScopedLayer(tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(num_units=units) for units in units_list]))\n",
    "    \n",
    "def bidirectional_dynamic_CONV(fw_cell, bw_cell, out_features=64):\n",
    "    bidir_conv_lstm = td.Composition()\n",
    "    with bidir_conv_lstm.scope():        \n",
    "        fw_seq = td.Identity().reads(bidir_conv_lstm.input[0])\n",
    "        labels = (td.GetItem(1)>>td.Map(td.Metric(\"labels\"))>>td.Void()).reads(bidir_conv_lstm.input)\n",
    "        bw_seq = td.Slice(step=-1).reads(fw_seq)\n",
    "\n",
    "        forward_dir = (td.RNN(fw_cell) >> td.GetItem(0)).reads(fw_seq)\n",
    "        back_dir = (td.RNN(bw_cell) >> td.GetItem(0)).reads(bw_seq)\n",
    "        back_to_leftright = td.Slice(step=-1).reads(back_dir)\n",
    "        \n",
    "        output_transform = (td.Function(lambda x: tf.reshape(x, [-1,vsize*out_features])) >>\n",
    "                            td.FC(1, activation=None))\n",
    "        \n",
    "        bidir_common = (td.ZipWith(td.Concat() >> \n",
    "                                   output_transform >> \n",
    "                                   td.Metric('logits'))).reads(forward_dir, back_to_leftright)\n",
    "                    \n",
    "        bidir_conv_lstm.output.reads(bidir_common)\n",
    "    return bidir_conv_lstm\n",
    "\n",
    "\n",
    "def bidirectional_dynamic_FC(fw_cell, bw_cell, hidden):\n",
    "    bidir_conv_lstm = td.Composition()\n",
    "    with bidir_conv_lstm.scope():        \n",
    "        fw_seq = td.Identity().reads(bidir_conv_lstm.input[0])\n",
    "        labels = (td.GetItem(1)>>td.Map(td.Metric(\"labels\"))>>td.Void()).reads(bidir_conv_lstm.input)\n",
    "        bw_seq = td.Slice(step=-1).reads(fw_seq)\n",
    "\n",
    "        forward_dir = (td.RNN(fw_cell) >> td.GetItem(0)).reads(fw_seq)\n",
    "        back_dir = (td.RNN(bw_cell) >> td.GetItem(0)).reads(bw_seq)\n",
    "        back_to_leftright = td.Slice(step=-1).reads(back_dir)\n",
    "        \n",
    "        output_transform = td.FC(1, activation=None)\n",
    "        \n",
    "        bidir_common = (td.ZipWith(td.Concat() >> \n",
    "                                  output_transform >> td.Metric('logits'))).reads(forward_dir, back_to_leftright)\n",
    "        \n",
    "        bidir_conv_lstm.output.reads(bidir_common)\n",
    "    return bidir_conv_lstm\n",
    "\n",
    "CONV_data = td.Record((td.Map(td.Vector(vsize) >> td.Function(lambda x: tf.reshape(x, [-1,vsize,1]))),td.Map(td.Scalar())))\n",
    "CONV_model =  (CONV_data >>\n",
    "               bidirectional_dynamic_CONV(convLSTM_cell(vsize), convLSTM_cell(vsize)) >>\n",
    "               td.Void())\n",
    "\n",
    "FC_data = td.Record((td.Map(td.Vector(vsize)),td.Map(td.Scalar())))\n",
    "FC_model = (FC_data >>\n",
    "            bidirectional_dynamic_FC(multi_FC_cell([1000]*5), multi_FC_cell([1000]*5),1000) >>\n",
    "            td.Void())\n",
    "\n",
    "store = data(FLAGS.data_dir + FLAGS.data_type, FLAGS.truncate)\n",
    "\n",
    "if FLAGS.model == \"lstm\":\n",
    "    model = FC_model\n",
    "elif FLAGS.model == \"convlstm\":\n",
    "    model = CONV_model\n",
    "else:\n",
    "    raise NotImplemented\n",
    "    \n",
    "compiler = td.Compiler.create(model)\n",
    "logits = tf.squeeze(compiler.metric_tensors['logits'])\n",
    "labels = compiler.metric_tensors['labels']\n",
    "predictions = tf.nn.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = data(FLAGS.data_dir + FLAGS.data_type, FLAGS.truncate)\n",
    "def pad(record):\n",
    "    pads = ((FLAGS.truncate-len(record[1]), 0), (0, 0))\n",
    "    ins = np.pad(record[0], pad_width=pads, mode=\"constant\", constant_values=0)\n",
    "    outs = np.pad(record[1], pad_width=(FLAGS.truncate-len(record[1]), 0), mode=\"constant\", constant_values=0)\n",
    "    return (ins, outs)\n",
    "\n",
    "def get_padded_batch(dataset=\"train\"):\n",
    "    data = np.zeros([FLAGS.batch_size, FLAGS.truncate, vsize])\n",
    "    labels = np.zeros([FLAGS.batch_size, FLAGS.truncate, 1])\n",
    "    for i in range(FLAGS.batch_size):\n",
    "        sentence, label = pad(next(store.data[dataset]))\n",
    "        data[i] = sentence\n",
    "        labels[i, :, 0] = label\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, FLAGS.truncate, vsize))\n",
    "y = tf.placeholder(tf.int32, shape=(FLAGS.batch_size, FLAGS.truncate, 1))\n",
    "labels = y\n",
    "num_units = [100, 100, 1]\n",
    "with tf.variable_scope(\"fw\"):\n",
    "    fw_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(num_units=units) for units in num_units])\n",
    "with tf.variable_scope(\"bw\"):\n",
    "    bw_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(num_units=units) for units in num_units])\n",
    "\n",
    "with tf.variable_scope(\"fw\"):\n",
    "    fw_outputs, st1 = tf.nn.dynamic_rnn(cell=fw_cells, inputs=x, dtype=tf.float32)\n",
    "with tf.variable_scope(\"bw\"):\n",
    "    bw_outputs, st2 = tf.nn.dynamic_rnn(bw_cells, tf.reverse(x, axis=[1]), dtype=tf.float32)\n",
    "\n",
    "logits = fw_outputs + tf.reverse(bw_outputs, axis=[1])\n",
    "predictions = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = tf.reduce_mean(tf.abs(tf.subtract(logits, tf.cast(labels, tf.float32))))\n",
    "l2_loss = tf.reduce_mean(tf.abs(tf.subtract(logits, tf.cast(labels, tf.float32))))\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.cast(labels, tf.float32)))\n",
    "\n",
    "if FLAGS.loss == \"l1\":\n",
    "    loss = l1_loss\n",
    "elif FLAGS.loss == \"l2\":\n",
    "    loss = l2_loss\n",
    "elif FLAGS.loss == \"crossentropy\":\n",
    "    loss = cross_entropy\n",
    "else:\n",
    "    raise NotImplemented\n",
    "\n",
    "path = FLAGS.log_dir + FLAGS.run_name\n",
    "writer = tf.summary.FileWriter(path, graph=tf.get_default_graph())\n",
    "saver = tf.train.Saver(max_to_keep=20)\n",
    "tf.summary.scalar(\"batch_loss\", loss)\n",
    "\n",
    "#Accuracy\n",
    "acc = tf.reduce_sum(tf.cast(tf.equal(tf.less(0.5,predictions), tf.cast(labels, tf.bool)),tf.int32))*100/tf.size(labels)\n",
    "tf.summary.scalar(\"batch_accuracy\", acc)\n",
    "\n",
    "# Recall\n",
    "correct_trues = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(tf.less(0.5, predictions), tf.cast(labels, tf.bool)), tf.cast(labels, tf.bool)), tf.int32))\n",
    "all_trues = tf.reduce_sum(labels)\n",
    "recall = tf.cast(correct_trues,tf.float32) / tf.cast(all_trues, tf.float32)\n",
    "tf.summary.scalar(\"recall\", recall)\n",
    "         \n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "if FLAGS.optimizer == \"ADAM\":\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "elif FLAGS.optimizer == \"RMS\":\n",
    "    opt = tf.train.RMSPropOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "elif FLAGS.optimizer == \"SGD\":\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "else:\n",
    "    raise NotImplemented\n",
    "\n",
    "train_op = opt.minimize(loss)\n",
    "tf.global_variables_initializer()\n",
    "\n",
    "# validation summary:\n",
    "validation_loss_placeholder = tf.placeholder(tf.float32, name=\"validation\")\n",
    "validation_loss_summary = tf.summary.scalar('validation_loss', validation_loss_placeholder)\n",
    "test_loss_placeholder = tf.placeholder(tf.float32, name=\"test\")\n",
    "test_loss_summary = tf.summary.scalar('validation_loss', test_loss_placeholder)\n",
    "\n",
    "\n",
    "def get_metrics_on_dataset(dataset, train_step):\n",
    "    losses = []\n",
    "    accs = []\n",
    "    recalls = []\n",
    "    step = int(store.size[dataset] / FLAGS.batch_size)\n",
    "    for i in tqdm.trange(step):\n",
    "        b_data, b_label = get_padded_batch(\"validation\")\n",
    "        batch_loss, accuracy, rec = sess.run([loss, acc, recall], {x: b_data, y: b_label})\n",
    "        losses.append(batch_loss)\n",
    "        accs.append(accuracy)\n",
    "        recalls.append(rec)\n",
    "    \n",
    "    avg_loss = np.average(losses)\n",
    "    \n",
    "    if dataset == \"validation\":\n",
    "        valid_summary = sess.run(validation_loss_summary,feed_dict={validation_loss_placeholder: avg_loss})\n",
    "        writer.add_summary(valid_summary, train_step)\n",
    "    elif dataset == \"test\":\n",
    "        test_summary = session.run(test_loss_summary,feed_dict={test_loss_placeholder: avg_loss})\n",
    "        writer.add_summary(test_summary, train_step)\n",
    "\n",
    "    return np.average(losses), np.average(accs), np.average(recalls)\n",
    "    \n",
    "    \n",
    "class stopper():\n",
    "    def __init__(self, patience=20):\n",
    "        self.log = []\n",
    "        self.patience = patience\n",
    "        self.should_stop = False\n",
    "        \n",
    "    def add(self, value):\n",
    "        self.log.append(value)\n",
    "        return self.check()\n",
    "    \n",
    "    def check(self):\n",
    "        minimum = min(self.log)\n",
    "        errors = sum([1 if i>minimum else 0 for i in self.log[self.log.index(minimum):]])\n",
    "        if errors > self.patience:\n",
    "            self.should_stop = True\n",
    "        return self.should_stop\n",
    "    \n",
    "early = stopper(20)\n",
    "steps = FLAGS.epochs * int(store.size[\"train\"] / FLAGS.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/90630 [00:00<?, ?batches/s]\n",
      "  0%|          | 0/503 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 2/503 [00:00<00:29, 16.78it/s]\u001b[A\n",
      "  1%|          | 5/503 [00:00<00:25, 19.34it/s]\u001b[A\n",
      "  1%|▏         | 7/503 [00:00<00:26, 18.84it/s]\u001b[A\n",
      "  2%|▏         | 10/503 [00:00<00:25, 19.66it/s]\u001b[A\n",
      "  2%|▏         | 12/503 [00:00<00:25, 19.34it/s]\u001b[A\n",
      "  3%|▎         | 15/503 [00:00<00:24, 19.78it/s]\u001b[A\n",
      "  4%|▎         | 18/503 [00:00<00:24, 20.12it/s]\u001b[A\n",
      "  4%|▍         | 21/503 [00:01<00:23, 20.36it/s]\u001b[A\n",
      "  5%|▍         | 24/503 [00:01<00:23, 20.58it/s]\u001b[A\n",
      "  5%|▌         | 27/503 [00:01<00:22, 20.73it/s]\u001b[A\n",
      "  6%|▌         | 30/503 [00:01<00:23, 20.56it/s]\u001b[A\n",
      "  7%|▋         | 33/503 [00:01<00:22, 20.66it/s]\u001b[A\n",
      "  7%|▋         | 36/503 [00:01<00:22, 20.75it/s]\u001b[A\n",
      "  8%|▊         | 39/503 [00:01<00:22, 20.62it/s]\u001b[A\n",
      "  8%|▊         | 42/503 [00:02<00:22, 20.71it/s]\u001b[A\n",
      "  9%|▉         | 45/503 [00:02<00:22, 20.80it/s]\u001b[A\n",
      " 10%|▉         | 48/503 [00:02<00:21, 20.87it/s]\u001b[A\n",
      " 10%|█         | 51/503 [00:02<00:21, 20.76it/s]\u001b[A\n",
      " 11%|█         | 54/503 [00:02<00:21, 20.66it/s]\u001b[A\n",
      " 11%|█▏        | 57/503 [00:02<00:21, 20.72it/s]\u001b[A\n",
      " 12%|█▏        | 60/503 [00:02<00:21, 20.76it/s]\u001b[A\n",
      " 13%|█▎        | 63/503 [00:03<00:21, 20.83it/s]\u001b[A\n",
      " 13%|█▎        | 66/503 [00:03<00:20, 20.89it/s]\u001b[A\n",
      " 14%|█▎        | 69/503 [00:03<00:20, 20.95it/s]\u001b[A\n",
      " 14%|█▍        | 72/503 [00:03<00:20, 20.99it/s]\u001b[A\n",
      " 15%|█▍        | 75/503 [00:03<00:20, 21.03it/s]\u001b[A\n",
      " 16%|█▌        | 78/503 [00:03<00:20, 21.08it/s]\u001b[A\n",
      " 16%|█▌        | 81/503 [00:03<00:20, 21.01it/s]\u001b[A\n",
      " 17%|█▋        | 84/503 [00:03<00:19, 21.03it/s]\u001b[A\n",
      " 17%|█▋        | 87/503 [00:04<00:19, 21.05it/s]\u001b[A\n",
      " 18%|█▊        | 90/503 [00:04<00:19, 21.03it/s]\u001b[A\n",
      " 18%|█▊        | 93/503 [00:04<00:19, 21.06it/s]\u001b[A\n",
      " 19%|█▉        | 96/503 [00:04<00:19, 21.08it/s]\u001b[A\n",
      " 20%|█▉        | 99/503 [00:04<00:19, 21.11it/s]\u001b[A\n",
      " 20%|██        | 102/503 [00:04<00:18, 21.13it/s]\u001b[A\n",
      " 21%|██        | 105/503 [00:04<00:18, 21.15it/s]\u001b[A\n",
      " 21%|██▏       | 108/503 [00:05<00:18, 21.17it/s]\u001b[A\n",
      " 22%|██▏       | 111/503 [00:05<00:18, 21.10it/s]\u001b[A\n",
      " 23%|██▎       | 114/503 [00:05<00:18, 21.13it/s]\u001b[A\n",
      " 23%|██▎       | 117/503 [00:05<00:18, 21.14it/s]\u001b[A\n",
      " 24%|██▍       | 120/503 [00:05<00:18, 21.08it/s]\u001b[A\n",
      " 24%|██▍       | 123/503 [00:05<00:18, 21.10it/s]\u001b[A\n",
      " 25%|██▌       | 126/503 [00:05<00:17, 21.12it/s]\u001b[A\n",
      " 26%|██▌       | 129/503 [00:06<00:17, 21.14it/s]\u001b[A\n",
      " 26%|██▌       | 132/503 [00:06<00:17, 21.16it/s]\u001b[A\n",
      " 27%|██▋       | 135/503 [00:06<00:17, 21.11it/s]\u001b[A\n",
      " 27%|██▋       | 138/503 [00:06<00:17, 21.12it/s]\u001b[A\n",
      " 28%|██▊       | 141/503 [00:06<00:17, 21.06it/s]\u001b[A\n",
      " 29%|██▊       | 144/503 [00:06<00:17, 21.08it/s]\u001b[A\n",
      " 29%|██▉       | 147/503 [00:06<00:16, 21.10it/s]\u001b[A\n",
      " 30%|██▉       | 150/503 [00:07<00:16, 21.12it/s]\u001b[A\n",
      " 30%|███       | 153/503 [00:07<00:16, 21.13it/s]\u001b[A\n",
      " 31%|███       | 156/503 [00:07<00:16, 21.08it/s]\u001b[A\n",
      " 32%|███▏      | 159/503 [00:07<00:16, 21.11it/s]\u001b[A\n",
      " 32%|███▏      | 162/503 [00:07<00:16, 21.12it/s]\u001b[A\n",
      " 33%|███▎      | 165/503 [00:07<00:16, 21.08it/s]\u001b[A\n",
      " 33%|███▎      | 168/503 [00:07<00:15, 21.04it/s]\u001b[A\n",
      " 34%|███▍      | 171/503 [00:08<00:15, 20.99it/s]\u001b[A\n",
      " 34%|███▍      | 173/503 [00:08<00:15, 20.95it/s]\u001b[A\n",
      " 35%|███▍      | 175/503 [00:08<00:15, 20.91it/s]\u001b[A\n",
      " 35%|███▌      | 178/503 [00:08<00:15, 20.93it/s]\u001b[A\n",
      " 36%|███▌      | 181/503 [00:08<00:15, 20.94it/s]\u001b[A\n",
      " 37%|███▋      | 184/503 [00:08<00:15, 20.96it/s]\u001b[A\n",
      " 37%|███▋      | 187/503 [00:08<00:15, 20.98it/s]\u001b[A\n",
      " 38%|███▊      | 190/503 [00:09<00:14, 20.99it/s]\u001b[A\n",
      " 38%|███▊      | 193/503 [00:09<00:14, 21.01it/s]\u001b[A\n",
      " 39%|███▉      | 196/503 [00:09<00:14, 21.02it/s]\u001b[A\n",
      " 40%|███▉      | 199/503 [00:09<00:14, 20.99it/s]\u001b[A\n",
      " 40%|████      | 202/503 [00:09<00:14, 21.00it/s]\u001b[A\n",
      " 41%|████      | 205/503 [00:09<00:14, 21.01it/s]\u001b[A\n",
      " 41%|████▏     | 208/503 [00:09<00:14, 20.98it/s]\u001b[A\n",
      " 42%|████▏     | 211/503 [00:10<00:13, 20.98it/s]\u001b[A\n",
      " 43%|████▎     | 214/503 [00:10<00:13, 20.97it/s]\u001b[A\n",
      " 43%|████▎     | 217/503 [00:10<00:13, 20.99it/s]\u001b[A\n",
      " 44%|████▎     | 220/503 [00:10<00:13, 21.00it/s]\u001b[A\n",
      " 44%|████▍     | 223/503 [00:10<00:13, 21.01it/s]\u001b[A\n",
      " 45%|████▍     | 226/503 [00:10<00:13, 21.03it/s]\u001b[A\n",
      " 46%|████▌     | 229/503 [00:10<00:13, 21.04it/s]\u001b[A\n",
      " 46%|████▌     | 232/503 [00:11<00:12, 21.05it/s]\u001b[A\n",
      " 47%|████▋     | 235/503 [00:11<00:12, 21.07it/s]\u001b[A\n",
      " 47%|████▋     | 238/503 [00:11<00:12, 21.08it/s]\u001b[A\n",
      " 48%|████▊     | 241/503 [00:11<00:12, 21.05it/s]\u001b[A\n",
      " 49%|████▊     | 244/503 [00:11<00:12, 21.06it/s]\u001b[A\n",
      " 49%|████▉     | 247/503 [00:11<00:12, 21.07it/s]\u001b[A\n",
      " 50%|████▉     | 250/503 [00:11<00:12, 21.08it/s]\u001b[A\n",
      " 50%|█████     | 253/503 [00:12<00:11, 21.02it/s]\u001b[A\n",
      " 51%|█████     | 256/503 [00:12<00:11, 21.03it/s]\u001b[A\n",
      " 51%|█████▏    | 259/503 [00:12<00:11, 21.01it/s]\u001b[A\n",
      " 52%|█████▏    | 262/503 [00:12<00:11, 20.99it/s]\u001b[A\n",
      " 53%|█████▎    | 265/503 [00:12<00:11, 21.00it/s]\u001b[A\n",
      " 53%|█████▎    | 268/503 [00:12<00:11, 20.98it/s]\u001b[A\n",
      " 54%|█████▍    | 271/503 [00:12<00:11, 20.95it/s]\u001b[A\n",
      " 54%|█████▍    | 273/503 [00:13<00:10, 20.92it/s]\u001b[A\n",
      " 55%|█████▍    | 275/503 [00:13<00:10, 20.90it/s]\u001b[A\n",
      " 55%|█████▌    | 278/503 [00:13<00:10, 20.91it/s]\u001b[A\n",
      " 56%|█████▌    | 280/503 [00:13<00:10, 20.88it/s]\u001b[A\n",
      " 56%|█████▋    | 283/503 [00:13<00:10, 20.89it/s]\u001b[A\n",
      " 57%|█████▋    | 286/503 [00:13<00:10, 20.90it/s]\u001b[A\n",
      " 57%|█████▋    | 289/503 [00:13<00:10, 20.91it/s]\u001b[A\n",
      " 58%|█████▊    | 292/503 [00:13<00:10, 20.88it/s]\u001b[A\n",
      " 59%|█████▊    | 295/503 [00:14<00:09, 20.86it/s]\u001b[A\n",
      " 59%|█████▉    | 298/503 [00:14<00:09, 20.88it/s]\u001b[A\n",
      " 60%|█████▉    | 301/503 [00:14<00:09, 20.89it/s]\u001b[A\n",
      " 60%|██████    | 304/503 [00:14<00:09, 20.87it/s]\u001b[A\n",
      " 61%|██████    | 307/503 [00:14<00:09, 20.88it/s]\u001b[A\n",
      " 62%|██████▏   | 310/503 [00:14<00:09, 20.87it/s]\u001b[A\n",
      " 62%|██████▏   | 313/503 [00:15<00:09, 20.82it/s]\u001b[A\n",
      " 63%|██████▎   | 316/503 [00:15<00:08, 20.83it/s]\u001b[A\n",
      " 63%|██████▎   | 319/503 [00:15<00:08, 20.84it/s]\u001b[A\n",
      " 64%|██████▍   | 322/503 [00:15<00:08, 20.86it/s]\u001b[A\n",
      " 65%|██████▍   | 325/503 [00:15<00:08, 20.87it/s]\u001b[A\n",
      " 65%|██████▌   | 328/503 [00:15<00:08, 20.88it/s]\u001b[A\n",
      " 66%|██████▌   | 331/503 [00:15<00:08, 20.86it/s]\u001b[A\n",
      " 66%|██████▋   | 334/503 [00:16<00:08, 20.86it/s]\u001b[A\n",
      " 67%|██████▋   | 337/503 [00:16<00:07, 20.87it/s]\u001b[A\n",
      " 68%|██████▊   | 340/503 [00:16<00:07, 20.87it/s]\u001b[A\n",
      " 68%|██████▊   | 343/503 [00:16<00:07, 20.88it/s]\u001b[A\n",
      " 69%|██████▉   | 346/503 [00:16<00:07, 20.89it/s]\u001b[A\n",
      " 69%|██████▉   | 349/503 [00:16<00:07, 20.90it/s]\u001b[A\n",
      " 70%|██████▉   | 352/503 [00:16<00:07, 20.91it/s]\u001b[A\n",
      " 71%|███████   | 355/503 [00:16<00:07, 20.92it/s]\u001b[A\n",
      " 71%|███████   | 358/503 [00:17<00:06, 20.93it/s]\u001b[A\n",
      " 72%|███████▏  | 361/503 [00:17<00:06, 20.94it/s]\u001b[A\n",
      " 72%|███████▏  | 364/503 [00:17<00:06, 20.95it/s]\u001b[A\n",
      " 73%|███████▎  | 367/503 [00:17<00:06, 20.96it/s]\u001b[A\n",
      " 74%|███████▎  | 370/503 [00:17<00:06, 20.97it/s]\u001b[A\n",
      " 74%|███████▍  | 373/503 [00:17<00:06, 20.98it/s]\u001b[A\n",
      " 75%|███████▍  | 376/503 [00:17<00:06, 20.96it/s]\u001b[A\n",
      " 75%|███████▌  | 379/503 [00:18<00:05, 20.97it/s]\u001b[A\n",
      " 76%|███████▌  | 382/503 [00:18<00:05, 20.95it/s]\u001b[A\n",
      " 77%|███████▋  | 385/503 [00:18<00:05, 20.94it/s]\u001b[A\n",
      " 77%|███████▋  | 388/503 [00:18<00:05, 20.95it/s]\u001b[A\n",
      " 78%|███████▊  | 391/503 [00:18<00:05, 20.96it/s]\u001b[A\n",
      " 78%|███████▊  | 394/503 [00:18<00:05, 20.97it/s]\u001b[A\n",
      " 79%|███████▉  | 397/503 [00:18<00:05, 20.97it/s]\u001b[A\n",
      " 80%|███████▉  | 400/503 [00:19<00:04, 20.98it/s]\u001b[A\n",
      " 80%|████████  | 403/503 [00:19<00:04, 20.99it/s]\u001b[A\n",
      " 81%|████████  | 406/503 [00:19<00:04, 20.92it/s]\u001b[A\n",
      " 81%|████████  | 408/503 [00:19<00:04, 20.91it/s]\u001b[A\n",
      " 82%|████████▏ | 411/503 [00:19<00:04, 20.91it/s]\u001b[A\n",
      " 82%|████████▏ | 413/503 [00:19<00:04, 20.89it/s]\u001b[A\n",
      " 83%|████████▎ | 416/503 [00:19<00:04, 20.90it/s]\u001b[A\n",
      " 83%|████████▎ | 419/503 [00:20<00:04, 20.91it/s]\u001b[A\n",
      " 84%|████████▍ | 422/503 [00:20<00:03, 20.87it/s]\u001b[A\n",
      " 84%|████████▍ | 425/503 [00:20<00:03, 20.88it/s]\u001b[A\n",
      " 85%|████████▌ | 428/503 [00:20<00:03, 20.86it/s]\u001b[A\n",
      " 85%|████████▌ | 430/503 [00:20<00:03, 20.82it/s]\u001b[A\n",
      " 86%|████████▌ | 432/503 [00:20<00:03, 20.81it/s]\u001b[A\n",
      " 86%|████████▋ | 435/503 [00:20<00:03, 20.79it/s]\u001b[A\n",
      " 87%|████████▋ | 438/503 [00:21<00:03, 20.80it/s]\u001b[A\n",
      " 88%|████████▊ | 441/503 [00:21<00:02, 20.81it/s]\u001b[A\n",
      " 88%|████████▊ | 444/503 [00:21<00:02, 20.82it/s]\u001b[A\n",
      " 89%|████████▉ | 447/503 [00:21<00:02, 20.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 450/503 [00:21<00:02, 20.84it/s]\u001b[A\n",
      " 90%|█████████ | 453/503 [00:21<00:02, 20.84it/s]\u001b[A\n",
      " 91%|█████████ | 456/503 [00:21<00:02, 20.84it/s]\u001b[A\n",
      " 91%|█████████▏| 459/503 [00:22<00:02, 20.84it/s]\u001b[A\n",
      " 92%|█████████▏| 462/503 [00:22<00:01, 20.85it/s]\u001b[A\n",
      " 92%|█████████▏| 465/503 [00:22<00:01, 20.81it/s]\u001b[A\n",
      " 93%|█████████▎| 468/503 [00:22<00:01, 20.80it/s]\u001b[A\n",
      " 94%|█████████▎| 471/503 [00:22<00:01, 20.79it/s]\u001b[A\n",
      " 94%|█████████▍| 474/503 [00:22<00:01, 20.80it/s]\u001b[A\n",
      " 95%|█████████▍| 477/503 [00:22<00:01, 20.80it/s]\u001b[A\n",
      " 95%|█████████▌| 480/503 [00:23<00:01, 20.81it/s]\u001b[A\n",
      " 96%|█████████▌| 483/503 [00:23<00:00, 20.82it/s]\u001b[A\n",
      " 97%|█████████▋| 486/503 [00:23<00:00, 20.80it/s]\u001b[A\n",
      " 97%|█████████▋| 489/503 [00:23<00:00, 20.79it/s]\u001b[A\n",
      " 98%|█████████▊| 492/503 [00:23<00:00, 20.80it/s]\u001b[A\n",
      " 98%|█████████▊| 495/503 [00:23<00:00, 20.81it/s]\u001b[A\n",
      " 99%|█████████▉| 498/503 [00:23<00:00, 20.81it/s]\u001b[A\n",
      "100%|█████████▉| 501/503 [00:24<00:00, 20.80it/s]\u001b[A\n",
      "100%|██████████| 503/503 [00:24<00:00, 20.81it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.660475  accuracy:  94.1204854208 % recall:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 179/90630 [00:54<7:43:01,  3.26batches/s] "
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "for i in tqdm.trange(steps, unit=\"batches\"):\n",
    "    b_data, b_label = get_padded_batch(\"train\")\n",
    "    _, batch_loss, summary = sess.run([train_op, loss, summary_op], {x: b_data, y: b_label})\n",
    "    assert not np.isnan(batch_loss)\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        writer.add_summary(summary, i)\n",
    "        \n",
    "    if i % 500 == 0:\n",
    "        l, a, r = get_metrics_on_dataset(\"validation\", i)\n",
    "        print(\"loss: \", l, \" accuracy: \", a, \"% recall: \", r)\n",
    "        if early.add(l):\n",
    "            break\n",
    "            \n",
    "    if i % 1000 == 0:\n",
    "        save_path = saver.save(sess, path + \"/model.ckpt\", global_step=i)\n",
    "        \n",
    "print(\"Testing...\")\n",
    "l, a, r = get_metrics_on_dataset(\"test\", steps)\n",
    "print(\"loss: \", l, \" accuracy: \", a, \"% recall: \", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
