{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for Tensorflow Fold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = td.Length()\n",
    "\n",
    "embedded =  (td.InputTransform(lambda s: [index(x) for x in s]) >> \n",
    "             td.Map(td.Scalar(tf.int32) >> \n",
    "             td.Function(td.Embedding(vsize, EMBEDDING_SIZE))))\n",
    "\n",
    "onehot = (td.InputTransform(lambda s: [index(x) for x in s]) >>\n",
    "          td.Map(td.Scalar(tf.int32) >>\n",
    "          td.Function(lambda indices: tf.one_hot(indices, depth=vsize)) >>\n",
    "          td.Function(lambda x: tf.reshape(x, [-1,vsize,1]))))\n",
    "\n",
    "decode_onehot = td.InputTransform(lambda s: [char(np.argmax(np.squeeze(x))) for x in s])\n",
    "\n",
    "print(decode_onehot.eval(onehot.eval(\"malÃº\")))\n",
    "#embedded.eval(\"kacsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_on_sequence(x, scope, kernel_size=3, input_channels=72, output_channels=72):\n",
    "    with tf.variable_scope(scope) as sc:\n",
    "        filters = tf.get_variable(\"conv_filter\", [kernel_size] +  [input_channels, output_channels] , initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        bias = tf.get_variable(\"conv_bias\",  output_channels, initializer=tf.constant_initializer(0.05, dtype=tf.float32))\n",
    "        conv = tf.nn.conv1d(x, filters=filters, stride=1, padding='VALID')\n",
    "        return tf.nn.relu(tf.add(conv, bias))\n",
    "    \n",
    "def SeqToTuple(T, N):\n",
    "    return (td.InputTransform(lambda x: tuple(x))\n",
    "            .set_input_type(td.SequenceType(T))\n",
    "            .set_output_type(td.Tuple(*([T] * N))))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_conv_LSTM():\n",
    "    convlstm = Conv1DLSTMCell(input_shape=[vsize,1], output_channels=8, kernel_shape=[5])\n",
    "    conv_lstm_cell_1d = td.ScopedLayer(convlstm)\n",
    "\n",
    "    bidir_conv_lstm = td.Composition()\n",
    "    with bidir_conv_lstm.scope():\n",
    "        data = td.Record((td.Map(\n",
    "                                td.Vector(vsize) >>\n",
    "                                td.Function(lambda x: tf.reshape(x, [-1,vsize,1]))),\n",
    "                          td.Map(\n",
    "                                td.Vector(vsize) >>\n",
    "                                td.Function(lambda x: tf.reshape(x, [-1,vsize,1]))))).reads(bidir_conv_lstm.input)\n",
    "\n",
    "        forward = td.Identity().reads(data[0])\n",
    "        backward = td.Identity().reads(data[1])\n",
    "\n",
    "        forw = (td.RNN(conv_lstm_cell_1d) >>\n",
    "                td.GetItem(1) >>\n",
    "                td.GetItem(0) >>\n",
    "                td.Function(lambda rnn_outs: tf.contrib.layers.flatten(rnn_outs))).reads(forward)\n",
    "\n",
    "        backw = (td.RNN(conv_lstm_cell_1d) >>\n",
    "                 td.GetItem(1) >>\n",
    "                 td.GetItem(0) >>\n",
    "                 td.Function(lambda rnn_outs: tf.contrib.layers.flatten(rnn_outs))).reads(backward)\n",
    "\n",
    "        rnn_outs = td.Concat().reads(forw,backw)\n",
    "        bidir_conv_lstm.output.reads(rnn_outs)\n",
    "    return bidir_conv_lstm >> td.FC(1)\n",
    "\n",
    "\n",
    "def FCNN():\n",
    "    return td.FC(50) >> td.FC(1)# >> td.Function(lambda xs: tf.squeeze(xs, axis=1))\n",
    "\n",
    "#bidir = bidirectional_conv_LSTM()\n",
    "#fc = FCNN()\n",
    "#blk = bidir# >> fc \n",
    "#blk.eval([a,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = td.ScopedLayer(tf.contrib.rnn.BasicLSTMCell(num_units=16), 'char_cell')\n",
    "blk = td.Map(td.Vector(vsize) >> td.Function(lambda x: tf.reshape(x, [-1,vsize]))) >> td.RNN(cell) >> td.GetItem(1) >> td.GetItem(0) >> td.FC(1)#>> td.Function(lambda xs: tf.squeeze(xs, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reader():\n",
    "    def __init__(self, folder, mode=\"bidirectional\", qsize=10000, start=True):\n",
    "        self.data_dir = folder\n",
    "        self.qsize= qsize\n",
    "        self.pool = dict()\n",
    "        self.data = dict()\n",
    "        self.datasets = [\"train\", \"test\", \"validation\"]\n",
    "        if mode == \"bidirectional\":\n",
    "            convert_fn = self.bidirectional_sentence_reader\n",
    "        else:\n",
    "            convert_fn = self.sentence_reader\n",
    "            \n",
    "        for dataset in self.datasets:\n",
    "            self.data[dataset] = Queue(qsize)\n",
    "            self.pool[dataset] = Process(target=convert_fn, args=(self.data[dataset], self.data_dir+dataset))\n",
    "        if start == True:\n",
    "            self.start()\n",
    "            \n",
    "    def sentence_reader(self, queue, file):\n",
    "        \"\"\"\n",
    "        read sentences from the data format setence: word\\tword\\n.....\\t\\n\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            with open(file) as f:\n",
    "                while True:\n",
    "                    try:\n",
    "                        sentence = []\n",
    "                        while True:\n",
    "                            line = f.readline()[:-1].split('\\t')\n",
    "                            if line[0] != \"\":\n",
    "                                sentence.append(line)\n",
    "                            else:\n",
    "                                break\n",
    "                        sent = \" \".join([word[0] for word in sentence])\n",
    "                        segmented = \" \".join([word[1].replace(\" \",\"|\") for word in sentence])\n",
    "                        tags = []\n",
    "                        last_char = \"_\"\n",
    "                        for char in segmented:\n",
    "                            if char != \"|\":\n",
    "                                tags.append(0 if last_char!=\"|\" else 1)\n",
    "                            last_char = char\n",
    "                        queue.put((sent, tags))\n",
    "                    except e:\n",
    "                        print(e)\n",
    "                        \n",
    "    def bidirectional_sentence_reader(self, queue, file):\n",
    "        \"\"\"\n",
    "        read sentences from the data format setence: word\\tword\\n.....\\t\\n\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            with open(file) as f:\n",
    "                while True:\n",
    "                    try:\n",
    "                        sentence = []\n",
    "                        while True:\n",
    "                            line = f.readline()[:-1].split('\\t')\n",
    "                            if line[0] != \"\":\n",
    "                                sentence.append(line)\n",
    "                            else:\n",
    "                                break\n",
    "                        sent = \" \".join([word[0] for word in sentence])\n",
    "                        segmented = \" \".join([word[1].replace(\" \",\"|\") for word in sentence])\n",
    "                        tags = []\n",
    "                        last_char = \"_\"\n",
    "                        for char in segmented:\n",
    "                            if char != \"|\":\n",
    "                                tags.append(0 if last_char!=\"|\" else 1)\n",
    "                            last_char = char\n",
    "                        for i in range(1, len(sent)-1):\n",
    "                            forward, backward = sent[:i], sent[i:][::-1]\n",
    "                            queue.put(([self.onehot(forward), self.onehot(backward)], tags[i]))\n",
    "                    except:\n",
    "                        print(\"err\")\n",
    "                        \n",
    "                        \n",
    "    def start(self):\n",
    "        for dataset in self.datasets:\n",
    "            self.pool[dataset].start()\n",
    "            \n",
    "    def stop(self):\n",
    "        for dataset in self.datasets:\n",
    "            self.pool[dataset].terminate()\n",
    "            \n",
    "    def get(self,dataset):\n",
    "        if dataset in self.datasets:\n",
    "            return self.data[dataset].get()\n",
    "        else:\n",
    "            raise KeyError\n",
    "            \n",
    "    def onehot(self, string):\n",
    "        onehot = np.zeros([len(string),vsize])\n",
    "        onehot[np.arange(len(string)), np.array([index(char) for char in string])]=1\n",
    "        return [onehot[i,:] for i in range(len(onehot))]\n",
    "            \n",
    "#store = reader(\"/home/moon/data/\", start=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
